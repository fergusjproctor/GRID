[data_preprocessor]
# Show the progress bar during the run
preprocessor_show_progress_bar = True

# maximum number of node of scene graph
max_node_number_sg = 87 # 87 # 42

# Maximum number of features in graphs
max_node_feature_number = 15 # 15 # 10

# Maximum number of words in the sequence
max_sequence_length = 40

# Define the method to process node features, options: 'onehot', 'sentence', 'tokenize'
# Default 'tokenize', Forced to be 'tokenize' when preprocessed_language = True
process_node_feature_method = 'tokenize'

# Define the method to process instruction, options: 'sentence', 'tokenize'
# Default 'tokenize', Forced to be 'tokenize' when preprocessed_language = True
process_instruction_method = 'tokenize'

# Define if the language should be preprocessed into tensor of features
preprocessed_language = True

[dataloader]
use_preprocessed_data = True
num_workers = 1

[training]
# Number of samples in dataset
dataset_size = 40000 # This can be set to 0 during preprocessing, but cannot be set to 0 when running on the network, because this number is needed when lrschedule is calculated.

# Number of batch size
batch_size = 240

# maximum epoch for training
max_epoch = 10 # was originally 700

# Visualize network, this requires graphviz package to be install. Check installation with 'dot -V'
visualize_network_flag = False

# id of sample to visualize prediction
vis_sample_id = 0

# Define should we split training and testing sets
train_test_split_enable = True

# Define the ratio of training set size over the data size 
train_split_ratio = 0.8


[loss]
action_loss_scale = 5
object_loss_scale = 25

# weight decay for both L1 and L2
weight_decay = 0.1

# enable L1
enable_l1 = True
l1_param = 0.2

# enable L2
enable_l2 = True
l2_param = 0.8

# maximum learning rate
max_lr = 1e-4


[litgrid]
# change language model for word tokenization, 2 choices: "instructor", "bert"
language_model = "instructor"

[grid]
# param for GRID
num_robot_node = 4
num_action = 10 # 10 # 4
d_model = 32
nhead = 4
num_encoder_layers = 4
num_decoder_layers = 4
dim_feedforward = 256
batch_first = True
bi_cross_nhead = 8
bi_cross_num_layers = 8

[instructor]
text_encoder_type = 'hkunlp/instructor-xl'
lm_encoded_gnn_flag = True
lm_word_embedding_dim = 1024
lm_sentence_embedding_dim = 768

# [bert]
## ONLY USED WHEN LM IS BERT
# text_encoder_type = "bert-base-uncased"
# padding = 'longest'
# return_tensors = 'pt'
# max_text_len = 256
# sub_sentence_present = True


[GCN]
# If the node is lm encoded
rg_encoder_in_channels = 1536
rg_encoder_hidden_channels_1 = 512
rg_encoder_out_channels = 256
# param for sg_encoder
sg_encoder_in_channels = 1536
sg_encoder_hidden_channels_1 = 512
sg_encoder_out_channels = 256